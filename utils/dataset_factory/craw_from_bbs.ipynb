{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import ConnectionError, HTTPError, Timeout, RequestException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://gmatclub.com/forum/actual-lsat-cr-bank-by-broall-249447.html?srsltid=AfmBOopTVwNhrVdTv_KFVTe69W7BTUYY9B36-2ziUSlcSXHUdM0olzUJ\"  # Replace with the actual URL\n",
    "\n",
    "# Send a GET request to fetch the webpage content\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Parse the webpage content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all <li> elements that contain the links\n",
    "li_elements = soup.find_all(\"li\", style=True)\n",
    "\n",
    "# Extract the <a> tag links from these <li> elements\n",
    "question_links = []\n",
    "for li in li_elements:\n",
    "    a_tag = li.find(\"a\", href=True)\n",
    "    if a_tag and \"gmatclub.com/forum\" in a_tag['href']:  # Filter for valid GMAT Club links\n",
    "        question_links.append(a_tag['href'])\n",
    "\n",
    "# Save the links to a file or display them\n",
    "with open(\"question_links.txt\", \"w\") as file:\n",
    "    for link in question_links:\n",
    "        file.write(link + \"\\n\")\n",
    "\n",
    "print(f\"Extracted {len(question_links)} question links. Saved to 'question_links.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Path to the file containing the list of links\n",
    "links_file = \"question_links.txt\"\n",
    "\n",
    "# Output CSV file\n",
    "output_file = \"questions_and_answers5.csv\"\n",
    "\n",
    "# Function to fetch a webpage with retries\n",
    "def fetch_page_with_retries(url, retries=3, backoff=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()  # Raise HTTPError for bad responses (4xx and 5xx)\n",
    "            return response\n",
    "        except ConnectionError as e:\n",
    "            print(f\"ConnectionError on attempt {attempt + 1} for {url}: {e}\")\n",
    "        except HTTPError as e:\n",
    "            print(f\"HTTPError on attempt {attempt + 1} for {url}: {e}\")\n",
    "        except Timeout as e:\n",
    "            print(f\"Timeout on attempt {attempt + 1} for {url}: {e}\")\n",
    "        except RequestException as e:\n",
    "            print(f\"RequestException on attempt {attempt + 1} for {url}: {e}\")\n",
    "        if attempt < retries - 1:\n",
    "            time.sleep(backoff)  # Exponential backoff\n",
    "    print(f\"Failed to fetch {url} after {retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# Function to scrape a single link\n",
    "def scrape_question_and_answer(url):\n",
    "    response = fetch_page_with_retries(url)\n",
    "    if response is None:\n",
    "        return None, None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # Find all <div> elements with class \"item text\"\n",
    "    item_texts = soup.find_all(\"div\", class_=\"item text\")\n",
    "\n",
    "    if not item_texts:\n",
    "        print(f\"No questions or answers found for {url}\")\n",
    "        return None, None\n",
    "\n",
    "    # Extract the question (first <div>)\n",
    "    question = item_texts[0].get_text(strip=True)\n",
    "\n",
    "    # Concatenate all remaining <div> elements for the answer\n",
    "    answer = \" \".join(item.get_text(strip=True) for item in item_texts[1:])\n",
    "\n",
    "    return question, answer\n",
    "\n",
    "# Read all links from the file\n",
    "with open(links_file, \"r\") as file:\n",
    "    links = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Open the output CSV file for writing\n",
    "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write the header row\n",
    "    writer.writerow([\"Question\", \"Answer\", \"URL\"])\n",
    "\n",
    "    # Process each link\n",
    "    for link in links:\n",
    "        print(f\"Processing: {link}\")\n",
    "        question, answer = scrape_question_and_answer(link)\n",
    "        if question and answer:\n",
    "            writer.writerow([question, answer, link])\n",
    "\n",
    "print(f\"Scraping completed. Extracted data saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged and deduplicated CSV saved to merged.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_and_deduplicate_csvs(file_paths, output_path):\n",
    "    \"\"\"\n",
    "    Merge multiple CSV files into one and remove duplicate rows.\n",
    "\n",
    "    Parameters:\n",
    "        file_paths (list of str): List of file paths to the CSV files to merge.\n",
    "        output_path (str): File path for the output merged and deduplicated CSV.\n",
    "    \n",
    "    Returns:\n",
    "        None: Writes the merged and deduplicated DataFrame to the specified output path.\n",
    "    \"\"\"\n",
    "    # Ensure the input is valid\n",
    "    if len(file_paths) != 5:\n",
    "        raise ValueError(\"Please provide exactly 5 CSV file paths.\")\n",
    "\n",
    "    # Load and concatenate CSVs\n",
    "    dataframes = [pd.read_csv(file) for file in file_paths]\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    deduplicated_df = merged_df.drop_duplicates()\n",
    "\n",
    "    # Save the deduplicated CSV\n",
    "    deduplicated_df.to_csv(output_path, index=False)\n",
    "    print(f\"Merged and deduplicated CSV saved to {output_path}\")\n",
    "# Example usage:\n",
    "file_paths = [\"questions_and_answers.csv\", \"questions_and_answers2.csv\", \"questions_and_answers3.csv\", \"questions_and_answers4.csv\", \"questions_and_answers5.csv\"]\n",
    "output_path = \"merged.csv\"\n",
    "merge_and_deduplicate_csvs(file_paths, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Baseurl = 'https://api.claudeshop.top'\n",
    "Skey =\"sk-EInQ3CxofU5UTNTkaB8QmIfn8Vh1jBYwILBc4nDNu0StzOLq\"\n",
    "\n",
    "headers = {\n",
    "   'Accept': 'application/json',\n",
    "   'Authorization': f'Bearer {Skey}',\n",
    "   'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',\n",
    "   'Content-Type': 'application/json'\n",
    "}\n",
    "# Function to call OpenAI API\n",
    "def process_with_openai(question, answer):\n",
    "    # Define the prompt for OpenAI\n",
    "    prompt = f\"\"\"\n",
    "\\nThe following text is extracted from a website. It contains a question, answer, some discussions about the answer, and some irrelevant text. Your task is to:\\n1. Filter out irrelevant text.\\n2. Extract the question and options.\\n3. Extract the correct answer.\\n4. Based on the explanation and discussion, explain the correct answer.\\nFormat the output as JSON with the following structure:\\n\"question\": \"The question and its options in a clean format,\\n    \"answer\": \"Explanation of the answer and the correct answer letter at the end\"\\nHere is the input text:\\nQuestion: {question}\\nAnswer and Discussions: {answer}\"\"\"\n",
    "    \n",
    "    url = Baseurl + \"/v1/chat/completions\"\n",
    "    payload = json.dumps({\n",
    "   \"model\": \"gpt-4o-2024-08-06\",\n",
    "   \"messages\": [\n",
    "      {\n",
    "         \"role\": \"system\",\n",
    "         \"content\": \"You are a helpful assistant for processing educational text.\"\n",
    "      },\n",
    "      {\n",
    "         \"role\": \"user\",\n",
    "         \"content\": prompt\n",
    "      }\n",
    "   ]\n",
    "})\n",
    "    # Call the OpenAI API\n",
    "    try:\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload) \n",
    "\n",
    "        # Extract the response content\n",
    "        content = response['choices'][0]['message']['content']\n",
    "        return content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing with OpenAI: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing question from URL: https://gmatclub.com/forum/before-the-printing-press-books-could-be-purchased-only-249446.html\n",
      "Error processing with OpenAI: HTTPSConnectionPool(host='api.claudeshop.top', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "Processing question from URL: https://gmatclub.com/forum/bevex-an-artificial-sweetener-used-only-in-soft-drinks-is-141503.html\n",
      "Error processing with OpenAI: HTTPSConnectionPool(host='api.claudeshop.top', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "Processing question from URL: https://gmatclub.com/forum/many-environmentalists-have-urged-environmental-awareness-249449.html\n",
      "Error processing with OpenAI: HTTPSConnectionPool(host='api.claudeshop.top', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "Processing question from URL: https://gmatclub.com/forum/historian-the-land-party-achieved-its-only-national-victory-143103.html\n",
      "Error processing with OpenAI: HTTPSConnectionPool(host='api.claudeshop.top', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "Processing question from URL: https://gmatclub.com/forum/hospital-executive-at-a-recent-conference-on-nonprofit-143234.html\n",
      "Error processing with OpenAI: HTTPSConnectionPool(host='api.claudeshop.top', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "Processing question from URL: https://gmatclub.com/forum/economist-every-business-strives-to-increase-its-productivi-163397.html\n",
      "Error processing with OpenAI: HTTPSConnectionPool(host='api.claudeshop.top', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "Processing question from URL: https://gmatclub.com/forum/a-century-in-certain-ways-is-like-a-life-163398.html\n",
      "Error processing with OpenAI: HTTPSConnectionPool(host='api.claudeshop.top', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "Processing question from URL: https://gmatclub.com/forum/consumer-the-latest-connorly-report-suggests-that-ocksenfre-163399.html\n",
      "Error processing with OpenAI: HTTPSConnectionPool(host='api.claudeshop.top', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "Processing question from URL: https://gmatclub.com/forum/scientist-earth-s-average-annual-temperature-has-increased-by-about-245247.html\n",
      "Error processing with OpenAI: HTTPSConnectionPool(host='api.claudeshop.top', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "Processing question from URL: https://gmatclub.com/forum/an-undergraduate-degree-245248.html\n",
      "Error processing with OpenAI: HTTPSConnectionPool(host='api.claudeshop.top', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "Processing completed. Data saved to head_preview.json.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# File paths\n",
    "input_csv = \"output.csv\"\n",
    "output_json = \"head_preview.json\"\n",
    "\n",
    "# Process the CSV and call OpenAI API for each entry\n",
    "processed_data = []\n",
    "\n",
    "with open(input_csv, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        question = row[\"Question\"]\n",
    "        answer = row[\"Answer\"]\n",
    "        url = row[\"URL\"]\n",
    "\n",
    "        print(f\"Processing question from URL: {url}\")\n",
    "        result = process_with_openai(question, answer)\n",
    "        if result:\n",
    "            try:\n",
    "                # Parse the result into JSON\n",
    "                result_json = json.loads(result)\n",
    "                result_json[\"source_url\"] = url  # Include source URL for reference\n",
    "                processed_data.append(result_json)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Failed to parse JSON for {url}: {e}\")\n",
    "\n",
    "# Save the processed data to a JSON file\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as jsonfile:\n",
    "    json.dump(processed_data, jsonfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Processing completed. Data saved to {output_json}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
