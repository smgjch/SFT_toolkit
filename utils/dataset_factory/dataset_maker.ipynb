{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def jsonl_to_json(jsonl_filename, json_filename):\n",
    "    with open(jsonl_filename, 'r') as jsonl_file:\n",
    "        json_array = []\n",
    "        for index, line in enumerate(jsonl_file):\n",
    "            json_object = json.loads(line)\n",
    "            json_object['ind'] = index\n",
    "            # if isinstance(json_object['label'], list) and len(json_object['label']) == 1:\n",
    "            #     json_object['label'] = json_object['label'][0]\n",
    "\n",
    "            json_array.append(json_object)\n",
    "    \n",
    "    with open(json_filename, 'w') as json_file:\n",
    "        json.dump(json_array, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "jsonl_to_json(f'logicReasoning/bigbench-logical-Args_bigbench-logical-args_test.jsonl', f'/home/recovery/Desktop/AGIEal_SFT_tools/logicReasoning/concated_LR_train.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed JSON has been saved to SFW_data_for_LSAT_AR_from_trian.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def transform_and_save_json(input_file, output_file):\n",
    "    # Read the input JSON from a file\n",
    "    with open(input_file, \"r\") as infile:\n",
    "        input_json = json.load(infile)\n",
    "    \n",
    "    output_json = []\n",
    "\n",
    "    for item in input_json:\n",
    "        context = item[\"context\"]\n",
    "        question = item[\"question\"]\n",
    "        choices = item[\"answers\"]\n",
    "        answer_text = item[\"label\"]\n",
    "        \n",
    "        # Construct the f-string\n",
    "        f_string = f\"{context}\\n{question}\\n\\n\"\n",
    "        options = []\n",
    "        answer_letter = \"\"\n",
    "        \n",
    "        # Assign letters to choices and construct options list\n",
    "        for idx, choice in enumerate(choices):\n",
    "            # print(\"idx and choice:\",idx,choice)\n",
    "            letter = chr(65 + idx )  # 'A', 'B', 'C', etc.\n",
    "            option = f\"{letter}. {choice}\\n\"\n",
    "            options.append(option)\n",
    "            # print(f\"answer_text {answer_text}, type {type(answer_text)}\")\n",
    "            if idx == answer_text:\n",
    "                answer_letter = f\"({letter})\"\n",
    "        \n",
    "        f_string += \"\".join(options)\n",
    "        \n",
    "        # Construct the message template\n",
    "        message = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f_string\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": answer_letter\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        output_json.append(message)\n",
    "    \n",
    "    # Write the transformed JSON to the output file\n",
    "    with open(output_file, \"w\") as outfile:\n",
    "        json.dump(output_json, outfile, indent=4)\n",
    "\n",
    "    print(f\"Transformed JSON has been saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_file_path = \"/home/recovery/Desktop/AGIEal_SFT_tools/data/analytical_reasoning/from_AGIEval_train/non_matching_contexts_ar.json\"  # Replace with your input file path\n",
    "output_file_path = \"SFW_data_for_LSAT_AR_from_trian.json\"  # Replace with your output file path\n",
    "\n",
    "transform_and_save_json(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed JSON has been saved to /home/recovery/Desktop/AGIEal_SFT_tools/SFT_data_for_LSAT_LR_detailed_reasoning_fromweb.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def transform_and_save_json(input_file, output_file):\n",
    "    # Read the input JSON from a file\n",
    "    with open(input_file, \"r\") as infile:\n",
    "        input_json = json.load(infile)\n",
    "    \n",
    "    output_json = []\n",
    "\n",
    "    for item in input_json:\n",
    "        background = item[\"background\"]\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "\n",
    "        \n",
    "        f_string = f\"Answer the following question. Background:{background}\\n Question: {question}\"\n",
    "        \n",
    "        # Construct the message template\n",
    "        message = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f_string\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": answer\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        output_json.append(message)\n",
    "    \n",
    "    # Write the transformed JSON to the output file\n",
    "    with open(output_file, \"w\") as outfile:\n",
    "        json.dump(output_json, outfile, indent=4)\n",
    "\n",
    "    print(f\"Transformed JSON has been saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_file_path = \"/home/recovery/Desktop/AGIEal_SFT_tools/SFT_data_for_LSAT_LR_explianed.json\"  # Replace with your input file path\n",
    "output_file_path = \"/home/recovery/Desktop/AGIEal_SFT_tools/SFT_data_for_LSAT_LR_detailed_reasoning_fromweb.json\"  # Replace with your output file path\n",
    "\n",
    "transform_and_save_json(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_last_five_paragraphs(input_string):\n",
    "    # Split the input string into paragraphs\n",
    "    paragraphs = input_string.split('\\n')\n",
    "    \n",
    "    # Check if there are at least five paragraphs\n",
    "    num_paragraphs = len(paragraphs)\n",
    "    start_index = max(0, num_paragraphs - 6)  # Calculate the starting index for the last five\n",
    "    \n",
    "    # Labels for the options\n",
    "    labels = ['A.', 'B.', 'C.', 'D.', 'E.']\n",
    "    \n",
    "    # Modify the last five paragraphs\n",
    "    for i in range(start_index, num_paragraphs):\n",
    "        label_index = i - start_index  # Calculate the label index\n",
    "        if label_index < len(labels):  # Check if we have enough labels\n",
    "            paragraphs[i] = labels[label_index] + ' ' + paragraphs[i]\n",
    "    \n",
    "    # Join the paragraphs back into a single string\n",
    "    output_string = '\\n'.join(paragraphs)\n",
    "    return output_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_and_replace_tabs_label(input_string):\n",
    "    input_string = label_last_five_paragraphs(input_string)\n",
    "    # Replace all tabs with spaces\n",
    "    no_tabs_string = input_string.replace('\\t', ' ')\n",
    "    \n",
    "    # Split the string into lines\n",
    "    lines = no_tabs_string.splitlines()\n",
    "    \n",
    "    # Join the lines using \\n as the separator\n",
    "    compressed_string = '\\\\n'.join(lines)\n",
    "    \n",
    "    return compressed_string\n",
    "def compress_and_replace_tabs(input_string):\n",
    "    # input_string = label_last_five_paragraphs(input_string)\n",
    "    # Replace all tabs with spaces\n",
    "    no_tabs_string = input_string.replace('\\t', ' ')\n",
    "    \n",
    "    # Split the string into lines\n",
    "    lines = no_tabs_string.splitlines()\n",
    "    \n",
    "    # Join the lines using \\n as the separator\n",
    "    compressed_string = '\\\\n'.join(lines)\n",
    "    \n",
    "    return compressed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"\"\"\n",
    "The following text is extracted from a website. It contains a question, answer, some discussions about the answer, and some irrelevant text. Your task is to:\n",
    "1. Filter out irrelevant text.\n",
    "2. Extract the question and options.\n",
    "3. Extract the correct answer.\n",
    "4. Based on the explanation and discussion, explain the correct answer.\n",
    "\n",
    "Format the output as JSON with the following structure:\n",
    "{\n",
    "    \"question\": \"The question and its options in a clean format,\n",
    "    \"answer\": \"Explanation of the answer and the correct answer letter at the end\"\n",
    "}\n",
    "\n",
    "Here is the input text:\n",
    "Question: {question}\n",
    "\n",
    "Answer and Discussions: {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nThe following text is extracted from a website. It contains a question, answer, some discussions about the answer, and some irrelevant text. Your task is to:\\n1. Filter out irrelevant text.\\n2. Extract the question and options.\\n3. Extract the correct answer.\\n4. Based on the explanation and discussion, explain the correct answer.\\n\\nFormat the output as JSON with the following structure:\\n{\\n    \"question\": \"The question and its options in a clean format,\\n    \"answer\": \"Explanation of the answer and the correct answer letter at the end\"\\n}\\nA. \\nB. Here is the input text:\\nC. Question: {question}\\nD. \\nE. Answer and Discussions: {answer}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "compressed_text = compress_and_replace_tabs_label(a)\n",
    "print(compressed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nThe following text is extracted from a website. It contains a question, answer, some discussions about the answer, and some irrelevant text. Your task is to:\\n1. Filter out irrelevant text.\\n2. Extract the question and options.\\n3. Extract the correct answer.\\n4. Based on the explanation and discussion, explain the correct answer.\\n\\nFormat the output as JSON with the following structure:\\n{\\n    \"question\": \"The question and its options in a clean format,\\n    \"answer\": \"Explanation of the answer and the correct answer letter at the end\"\\n}\\n\\nHere is the input text:\\nQuestion: {question}\\n\\nAnswer and Discussions: {answer}\n"
     ]
    }
   ],
   "source": [
    "compressed_text = compress_and_replace_tabs(a)\n",
    "print(compressed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def process_file(input_file_path, output_file_path):\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        skip_next = False\n",
    "        for i in range(len(lines)):\n",
    "            if skip_next:\n",
    "                skip_next = False\n",
    "                continue\n",
    "            if lines[i].startswith(\"Question Difficulty\"):\n",
    "                skip_next = True\n",
    "                continue\n",
    "            if lines[i].startswith(\"![](\"):\n",
    "                skip_next = False\n",
    "                continue\n",
    "            if lines[i].startswith(\"# ID:\") and lines[i].endswith(\"Answer  \\n\"):\n",
    "                skip_next = False\n",
    "                continue\n",
    "            file.write(lines[i])\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    # Check if output directory exists, if not, create it\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Process each file in the input directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".md\"):\n",
    "            input_file_path = os.path.join(input_dir, filename)\n",
    "            output_file_path = os.path.join(output_dir, filename)\n",
    "            process_file(input_file_path, output_file_path)\n",
    "\n",
    "# Example usage\n",
    "input_directory = '/home/recovery/Desktop/AGIEal_SFT_tools/data/sat_en/from_SAT_official/parsed_markdown'\n",
    "output_directory = '/home/recovery/Desktop/AGIEal_SFT_tools/data/sat_en/from_SAT_official/filtered'\n",
    "process_directory(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_md_to_json(md_content):\n",
    "    # Split the content by lines\n",
    "    lines = md_content.split('\\n')\n",
    "    \n",
    "    # Initialize a list to hold each parsed question block\n",
    "    parsed_data = []\n",
    "    current_block = None\n",
    "    current_role = None\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith('# ID:'):\n",
    "            if current_block:\n",
    "                parsed_data.append(current_block)\n",
    "            current_block = {'messages': []}\n",
    "            current_role = 'user'\n",
    "        elif line.startswith('Correct Answer:'):\n",
    "            current_role = 'assistant'\n",
    "            # Ensure there is already a user message to modify\n",
    "            if current_block and current_block['messages']:\n",
    "                current_block['messages'].append({\n",
    "                    \"role\": 'assistant',\n",
    "                    \"content\": \"\"\n",
    "                })\n",
    "        else:\n",
    "            if current_role and current_block:\n",
    "                # Add content to the current role's last message, or create new one if needed\n",
    "                if current_block['messages'] and current_block['messages'][-1]['role'] == current_role:\n",
    "                    current_block['messages'][-1]['content'] += line + '\\n'\n",
    "                else:\n",
    "                    current_block['messages'].append({\n",
    "                        \"role\": current_role,\n",
    "                        \"content\": line + '\\n'\n",
    "                    })\n",
    "    \n",
    "    # Append the last block if it exists\n",
    "    if current_block:\n",
    "        parsed_data.append(current_block)\n",
    "\n",
    "    return parsed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    input_directory = '/home/recovery/Desktop/AGIEal_SFT_tools/data/sat_en/from_SAT_official/filtered'\n",
    "    output_directory = '/home/recovery/Desktop/AGIEal_SFT_tools/data/sat_en/from_SAT_official'\n",
    "    prefix = 'SFT_data_for_SAT_en_from_SATofficial_'\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Process each file in the directory\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith('.md'):\n",
    "            input_path = os.path.join(input_directory, filename)\n",
    "            output_filename = prefix + filename.replace('.md', '.json')\n",
    "            output_path = os.path.join(output_directory, output_filename)\n",
    "            # print(output_path)\n",
    "            # Read markdown content\n",
    "            with open(input_path, 'r', encoding='utf-8') as file:\n",
    "                md_content = file.read()\n",
    "\n",
    "            # Parse markdown content to JSON structure\n",
    "            parsed_data = parse_md_to_json(md_content)\n",
    "            # Count the number of items in the parsed data\n",
    "            item_count = len(parsed_data)\n",
    "\n",
    "            # Construct the output filename with item count\n",
    "            output_filename = f\"{prefix}{filename[:-3]}_{item_count}.json\"\n",
    "            output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "            # Write JSON output\n",
    "            with open(output_path, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(parsed_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "            print(f\"Processed {filename} -> {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Form_Structure_and_Sense.md -> SFT_data_for_SAT_en_from_SATofficial_Form_Structure_and_Sense_51.json\n",
      "Processed Central_Ideas_and_Details.md -> SFT_data_for_SAT_en_from_SATofficial_Central_Ideas_and_Details_40.json\n",
      "Processed Boundaries.md -> SFT_data_for_SAT_en_from_SATofficial_Boundaries_49.json\n",
      "Processed Inferences.md -> SFT_data_for_SAT_en_from_SATofficial_Inferences_45.json\n",
      "Processed Text_Structure_and_Purpose.md -> SFT_data_for_SAT_en_from_SATofficial_Text_Structure_and_Purpose_43.json\n",
      "Processed Cross_Text_Connections.md -> SFT_data_for_SAT_en_from_SATofficial_Cross_Text_Connections_35.json\n",
      "Processed Rhetorical_Synthesis.md -> SFT_data_for_SAT_en_from_SATofficial_Rhetorical_Synthesis_51.json\n",
      "Processed Transitions.md -> SFT_data_for_SAT_en_from_SATofficial_Transitions_43.json\n",
      "Processed Words_in_Context.md -> SFT_data_for_SAT_en_from_SATofficial_Words_in_Context_53.json\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_txt_to_json(txt_content):\n",
    "    # Split the content into blocks by double newlines, assuming each block is a separate question\n",
    "    blocks = txt_content.strip().split('\\n\\n')\n",
    "    parsed_data = []\n",
    "\n",
    "    for block in blocks:\n",
    "        lines = block.strip().split('\\n')\n",
    "        if lines:\n",
    "            # The main question text is the first part\n",
    "            question_text = ' '.join(lines[:-4])  # Assuming the last four lines are the options\n",
    "            options = lines[-4:]  # The last four lines are the options\n",
    "\n",
    "            # Assume the correct answer is always the first option\n",
    "            correct_answer = question_text[0]  # 'A', 'B', 'C', 'D'\n",
    "\n",
    "            # Construct the messages\n",
    "            question_block = {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": question_text[1:] + \" \" + \" \".join(options)\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": f\"{correct_answer.upper()}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            parsed_data.append(question_block)\n",
    "\n",
    "    return parsed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train.txt -> SFT_data_for_LogiQA_from_LogiQAofficial_Train_8678.json\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    input_directory = '/home/recovery/Desktop/AGIEal_SFT_tools/data/LogiQA/from_official_train/'\n",
    "    output_directory = '/home/recovery/Desktop/AGIEal_SFT_tools/data/LogiQA/'\n",
    "    prefix = 'SFT_data_for_LogiQA_from_LogiQAofficial_'\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Process each file in the directory\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            input_path = os.path.join(input_directory, filename)\n",
    "            with open(input_path, 'r', encoding='utf-8') as file:\n",
    "                txt_content = file.read()\n",
    "\n",
    "            # Parse TXT content to JSON structure\n",
    "            parsed_data = parse_txt_to_json(txt_content)\n",
    "            item_count = len(parsed_data)\n",
    "\n",
    "            # Construct the output filename with item count\n",
    "            output_filename = f\"{prefix}{filename[:-4]}_{item_count}.json\"\n",
    "            output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "            # Write JSON output\n",
    "            with open(output_path, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(parsed_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "            print(f\"Processed {filename} -> {output_filename}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
