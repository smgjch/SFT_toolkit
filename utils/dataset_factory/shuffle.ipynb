{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/hwfile/opendatalab/mingchenlin/data/finalData', '/mnt/hwfile/opendatalab/panzhuoshi/data/liyu/husst/Husst_train_sampled_openai.json', '/mnt/hwfile/opendatalab/panzhuoshi/data/liyu/zebra/end_filter_data_2_openai.json', '/mnt/petrelfs/hujucheng/SFT_tools/SFT_toolkit/data/1219_1209_hu_enhenced', '/mnt/petrelfs/hujucheng/SFT_tools/SFT_toolkit/data/1223_hu_further_enhenced/hu_SFT_data_for_LSAT_LR_from_trian_1200.json', '/mnt/hwfile/opendatalab/peiqizhi/data/hucb/train.json', '/mnt/hwfile/opendatalab/peiqizhi/data/hucb/train.json', '/mnt/hwfile/opendatalab/peiqizhi/data/hucb/train.json', '/mnt/hwfile/opendatalab/tangzinan/multinli_emebed_top30_match_openai_2164.json', '/mnt/hwfile/opendatalab/peiqizhi/data/hurc/ans_train_sample5k.json', '/mnt/hwfile/opendatalab/peiqizhi/data/hucola/train_sample1_75p.json', '/mnt/hwfile/opendatalab/peiqizhi/data/hucola/human_1500.json', '/mnt/hwfile/opendatalab/panzhuoshi/data/gaoxin/translation_dataset', '/mnt/hwfile/opendatalab/gaoxin/trans_v1-plus', '/mnt/hwfile/opendatalab/gaoxin/word_dataset_v2', '/mnt/hwfile/opendatalab/panzhuoshi/data/gaoxin/translation_dataset', '/mnt/hwfile/opendatalab/gaoxin/trans_v1-plus', '/mnt/hwfile/opendatalab/gaoxin/word_dataset_v2', '/mnt/hwfile/opendatalab/gaoxin/trans_v1-max/en2hu2.jsonl', '/mnt/hwfile/opendatalab/gaoxin/trans_v1-max/en2hu2.jsonl', '/mnt/hwfile/opendatalab/gaoxin/trans_v1-max/en2hu2.jsonl', '/mnt/hwfile/opendatalab/gaoxin/trans_v1-max/zh2en2.jsonl', '/mnt/hwfile/opendatalab/gaoxin/trans_v1-max/zh2en2.jsonl', '/mnt/hwfile/opendatalab/gaoxin/trans_v1-max/zh2en2.jsonl', '/mnt/hwfile/opendatalab/gaoxin/trans_v1-max/zh2hu2.jsonl']\n",
      "DATA NUM: 149022\n",
      "Combined and shuffled data saved to /mnt/petrelfs/hujucheng/SFT_tools/SFT_toolkit/data/1226_to_train/1209_hu_further_enhenced_Tallx2_hucbx3_transMax_enhux3_zhenx3_zhzhu.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "random.seed(42)\n",
    "# Path to the configuration file\n",
    "cfg_file_path = \"/mnt/petrelfs/hujucheng/SFT_tools/SFT_toolkit/utils/dataset_factory/configs_1226/1209_hu_further_enhenced_Tallx2_hucbx3_transMax_enhux3_zhenx3_zhzhu.cfg\"\n",
    "\n",
    "# Function to parse the .cfg file\n",
    "def parse_cfg_file(cfg_path):\n",
    "    with open(cfg_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    # Extract the RAW_DATASETS and DATASETS paths using string manipulation\n",
    "    raw_datasets_start = content.find('RAW=\"') + len('RAW=\"')\n",
    "    raw_datasets_end = content.find('\"', raw_datasets_start)\n",
    "    raw_datasets = content[raw_datasets_start:raw_datasets_end].strip().split(\" \\\\\\n\")\n",
    "\n",
    "    datasets_start = content.find('DATASETS=\"') + len('DATASETS=\"')\n",
    "    datasets_end = content.find('\"', datasets_start)\n",
    "    datasets_path = content[datasets_start:datasets_end].strip()\n",
    "\n",
    "    return raw_datasets, datasets_path\n",
    "\n",
    "# Function to load data from a JSON file\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Parse the configuration file\n",
    "raw_datasets, datasets_path = parse_cfg_file(cfg_file_path)\n",
    "print(raw_datasets)\n",
    "combined_data = []\n",
    "\n",
    "# Loop through each dataset path in RAW_DATASETS\n",
    "for dataset in raw_datasets:\n",
    "    if os.path.isfile(dataset):  # If it's a file, load it directly\n",
    "        if dataset.endswith(\".json\") or dataset.endswith(\".jsonl\"):\n",
    "            combined_data.extend(load_json(dataset))\n",
    "    elif os.path.isdir(dataset):  \n",
    "        for root, dirs, files in os.walk(dataset):\n",
    "            if len(files) == 0:\n",
    "                print(f\"Please check {dataset}, error detected！！\")\n",
    "                break\n",
    "            for file in files:\n",
    "                if file.endswith(\".json\") or file.endswith(\".jsonl\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    combined_data.extend(load_json(file_path))\n",
    "                \n",
    "\n",
    "# Shuffle the combined data\n",
    "random.shuffle(combined_data)\n",
    "print(\"DATA NUM:\", len(combined_data))\n",
    "# Save the combined and shuffled data to the specified DATASETS path\n",
    "with open(datasets_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(combined_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Combined and shuffled data saved to {datasets_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation 5000\n",
    "Danyu 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA NUM for 1209_allx2_cbx3_V3_g13kx2_safe_base25w.cfg: 359145\n",
      "Combined and shuffled data saved to /mnt/petrelfs/hujucheng/SFT_tools/SFT_toolkit/data/1227_to_train/1209_allx2_cbx3_base25w_V3_g13kx2_safe.json\n",
      "DATA NUM for 1209_allx2_cbx3_V3_g13kx2_safe_base20w.cfg: 309145\n",
      "Combined and shuffled data saved to /mnt/petrelfs/hujucheng/SFT_tools/SFT_toolkit/data/1227_to_train/1209_allx2_cbx3_base20w_V3_g13kx2_safe.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Function to parse a .cfg file\n",
    "def parse_cfg_file(cfg_path):\n",
    "    with open(cfg_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    raw_datasets_start = content.find('RAW=\"') + len('RAW=\"')\n",
    "    raw_datasets_end = content.find('\"', raw_datasets_start)\n",
    "    raw_datasets = content[raw_datasets_start:raw_datasets_end].strip().split(\" \\\\\\n\")\n",
    "\n",
    "    datasets_start = content.find('DATASETS=\"') + len('DATASETS=\"')\n",
    "    datasets_end = content.find('\"', datasets_start)\n",
    "    datasets_path = content[datasets_start:datasets_end].strip()\n",
    "\n",
    "    return raw_datasets, datasets_path\n",
    "\n",
    "# Function to load data from a JSON file\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Function to process all .cfg files in a directory\n",
    "def process_cfg_files(directory):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.cfg'):\n",
    "                cfg_file_path = os.path.join(root, file)\n",
    "                raw_datasets, datasets_path = parse_cfg_file(cfg_file_path)\n",
    "                combined_data = []\n",
    "\n",
    "                # Process each dataset in RAW_DATASETS\n",
    "                for dataset in raw_datasets:\n",
    "                    if os.path.isfile(dataset):\n",
    "                        if dataset.endswith(\".json\") or dataset.endswith(\".jsonl\"):\n",
    "                            combined_data.extend(load_json(dataset))\n",
    "                    elif os.path.isdir(dataset):\n",
    "                        for dir_root, _, dir_files in os.walk(dataset):\n",
    "                            for dir_file in dir_files:\n",
    "                                if dir_file.endswith(\".json\") or dir_file.endswith(\".jsonl\"):\n",
    "                                    file_path = os.path.join(dir_root, dir_file)\n",
    "                                    combined_data.extend(load_json(file_path))\n",
    "\n",
    "                # Shuffle and save the combined data\n",
    "                random.shuffle(combined_data)\n",
    "                print(f\"DATA NUM for {file}:\", len(combined_data))\n",
    "                with open(datasets_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(combined_data, f, ensure_ascii=False, indent=4)\n",
    "                print(f\"Combined and shuffled data saved to {datasets_path}\")\n",
    "\n",
    "# Specify the directory containing .cfg files\n",
    "cfg_directory = \"/mnt/petrelfs/hujucheng/SFT_tools/SFT_toolkit/utils/dataset_factory/configs_1227\"\n",
    "\n",
    "# Process all .cfg files in the specified directory\n",
    "process_cfg_files(cfg_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocPlayground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
